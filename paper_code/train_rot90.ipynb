{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN vs HistNN vs CNN + HistNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main experiment script, which trains a CNN, a HistNN and a merged CNN/HistNN network on the same dataset.\n",
    "\n",
    "This is designed to be run using the `run_experiments.py` script so we get some config from envvars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load parameters from env variables\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "EXP_NAME = os.environ.get('EXP_NAME', 'labels_4_test_fold_0_rep_0')\n",
    "DEVICE = os.environ.get('DEVICE', 'gpu0')\n",
    "\n",
    "OUTDIR = os.environ.get(\n",
    "    'OUTDIR',\n",
    "    '../_out/%s_%s_%s_rot90' % (EXP_NAME, DEVICE, datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    ")\n",
    "\n",
    "CNN_NEPOCHS = int(os.environ.get('CNN_NEPOCHS', 50))\n",
    "HISTNN_NEPOCHS = int(os.environ.get('HISTNN_NEPOCHS', 50))\n",
    "MERGED_NEPOCHS = int(os.environ.get('MERGED_NEPOCHS', 50))\n",
    "KERAS_VERBOSE = int(os.environ.get('KERAS_VERBOSE', 1))\n",
    "\n",
    "print 'EXP_NAME :', EXP_NAME\n",
    "print 'DEVICE :', DEVICE\n",
    "print 'CNN_NEPOCHS : ', CNN_NEPOCHS\n",
    "print 'HISTNN_NEPOCHS : ', HISTNN_NEPOCHS\n",
    "print 'MERGED_NEPOCHS : ', MERGED_NEPOCHS\n",
    "print 'OUTDIR : ', OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano config\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=%s,nvcc.fastmath=True,floatX=float32' % DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import agronn.dataio as dataio\n",
    "import agronn.utils as utils\n",
    "import agronn.keras_utils as keras_utils\n",
    "import pylab as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.mkdir_p(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a dict where we'll store our results for later analysis\n",
    "log = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "winsize = 21\n",
    "nbins = 20\n",
    "# we work on centered RGB images\n",
    "binranges = [(-0.5, 0.5), (-0.5, 0.5), (-0.5, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mosaic, id2label, train_ij, test_ij, y_train, y_test, d = dataio.load(EXP_NAME, ret_d=True,\n",
    "                                                                      data_fname='data_rot90_final.hdf5')\n",
    "img = d['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_ij.shape\n",
    "print test_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"mosaic shape \", mosaic.shape\n",
    "#pl.imshow(img_mosaic[10000:15000])\n",
    "\n",
    "print \"mosaic takes \", mosaic.nbytes / (1024. * 1024.), \"mb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = utils.ImageCenterer().fit(img)\n",
    "mosaic_scaled = scaler.transform(mosaic).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils.layer_utils import print_layer_shapes\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "\n",
    "import keras.models as models\n",
    "\n",
    "#weights_init = 'uniform'\n",
    "weights_init = 'glorot_uniform'\n",
    "activation = 'relu'\n",
    "nclasses = len(id2label)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def eval_classif(m, ij, y):\n",
    "    y_pred = m.predict({'ij':ij})['output'].argmax(axis=1)\n",
    "    report = 'kappa : %f\\n' % utils.kappa(y, y_pred)\n",
    "    report += classification_report(y, y_pred, labels=np.arange(nclasses),\n",
    "                                    target_names=id2label)\n",
    "    return report, y_pred\n",
    "\n",
    "def eval_model(m):\n",
    "    # Don't evaluate on whole train, this takes too long\n",
    "    samples = np.random.choice(train_ij.shape[0], 20000)\n",
    "    train_report, y_train_pred = eval_classif(m, train_ij[samples], y_train[samples])    \n",
    "    test_report, y_test_pred = eval_classif(m, test_ij, y_test)\n",
    "    \n",
    "    d = {\n",
    "        'train_report' : train_report,\n",
    "        'test_report' : test_report,\n",
    "        'y_train_pred' : y_train_pred,\n",
    "        'y_train_true' : y_train[samples],\n",
    "        'y_test_pred' : y_test_pred,\n",
    "        'y_test_true' : y_test\n",
    "    }\n",
    "    return d\n",
    "\n",
    "def print_model_eval(m):\n",
    "    \"\"\"\n",
    "    Returns y_pred_train, y_true_train, y_pred_test, y_true_test\n",
    "    \"\"\"\n",
    "    d = eval_model(m)\n",
    "    print '-- train (sampled)'\n",
    "    print d['train_report']\n",
    "\n",
    "    print '-- test'\n",
    "    print d['test_report']\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Share the ExtractWindowsLayer because it takes GPU memory\n",
    "extract_win_layer = keras_utils.ExtractWindowsLayer(winsize, mosaic_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you'd do to extract the windows once, but this doesn't seem to change the training time much :\n",
    "\n",
    "\n",
    "    import agronn.my_ops as my_ops\n",
    "    import theano.tensor\n",
    "\n",
    "    img_t = theano.tensor.tensor3()\n",
    "    win_ij_t = theano.tensor.matrix(dtype='int32')\n",
    "\n",
    "    windows_2d = my_ops.extract_2d_windows(winsize)(img_t, win_ij_t)\n",
    "    extract_windows = theano.function(\n",
    "        [img_t, win_ij_t],\n",
    "        windows_2d,\n",
    "    )\n",
    "\n",
    "    train_win = extract_windows(mosaic_scaled, train_ij)\n",
    "    train_win = np.array(out_win)\n",
    "\n",
    "    print train_win.shape\n",
    "    print train_win.nbytes / (1024. * 1024.), ' mb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn_pretrain = models.Graph()\n",
    "cnn_pretrain.scaler = scaler\n",
    "cnn_pretrain.add_input(name='ij', input_shape=(2,))\n",
    "\n",
    "cnn_pretrain.add_node(extract_win_layer, name='extract_windows', input='ij')\n",
    "#cnn_pretrain.add_input(name='extract_windows', input_shape=(3, winsize, winsize))\n",
    "\n",
    "# cnn\n",
    "cnn = models.Graph()\n",
    "cnn.add_input(name='win', input_shape=(3, winsize, winsize))\n",
    "cnn.add_node(Convolution2D(48, 11, 11, border_mode='valid', init=weights_init),\n",
    "             name='conv1', input='win')\n",
    "cnn.add_node(Activation(activation), name='act1', input='conv1')\n",
    "cnn.add_node(MaxPooling2D(pool_size=(2,2)), name='pool1', input='act1')\n",
    "cnn.add_node(Dropout(0.25), name='dropout1', input='pool1')\n",
    "\n",
    "cnn.add_node(Convolution2D(48, 3, 3, border_mode='valid', init=weights_init), \n",
    "             name='conv2', input='dropout1')\n",
    "cnn.add_node(Activation(activation), name='act2', input='conv2')\n",
    "cnn.add_node(MaxPooling2D(pool_size=(2,2)), name='pool2', input='act2')\n",
    "cnn.add_node(Dropout(0.25), name='dropout2', input='pool2')\n",
    "cnn.add_node(Flatten(), name='flatten', input='dropout2')\n",
    "cnn.add_node(Dense(128, init=weights_init), name='dense1',\n",
    "                      input='flatten')\n",
    "cnn.add_node(Activation(activation), name='act3', input='dense1')\n",
    "cnn.add_node(Dropout(0.1), name='dropout3', input='act3')\n",
    "cnn.add_output(name='cnn_output', input='dropout3')\n",
    "\n",
    "print_layer_shapes(cnn, input_shapes={'win':(42, 3, 21, 21)})\n",
    "\n",
    "# -- pred\n",
    "cnn_pretrain.add_node(cnn, name='cnn', input='extract_windows')\n",
    "cnn_pretrain.add_node(Dense(nclasses, init=weights_init), name='dense2',\n",
    "                      input='cnn')\n",
    "cnn_pretrain.add_node(Activation('softmax'), name='softmax', input='dense2')\n",
    "cnn_pretrain.add_output(name='output', input='softmax')\n",
    "\n",
    "cnn_pretrain.compile('adam', {'output':'categorical_crossentropy'})\n",
    "\n",
    "print_layer_shapes(cnn_pretrain, input_shapes={'ij':train_ij[:42].shape})\n",
    "#print_layer_shapes(cnn_pretrain, input_shapes={'extract_windows':train_win[:42].shape})\n",
    "\n",
    "keras_utils.print_model_params(cnn_pretrain)\n",
    "\n",
    "cnn_pretrain_history = cnn_pretrain.fit(\n",
    "    {'ij': train_ij, 'output': Y_train},\n",
    "    #{'extract_windows' : train_win, 'output': Y_train},\n",
    "    batch_size=batch_size, nb_epoch=CNN_NEPOCHS,\n",
    "    verbose=KERAS_VERBOSE, shuffle=True\n",
    ")\n",
    "\n",
    "pl.figure()\n",
    "pl.title('Training loss')\n",
    "pl.plot(cnn_pretrain_history.epoch,\n",
    "        cnn_pretrain_history.history['loss'], label='loss')\n",
    "pl.savefig(os.path.join(OUTDIR, 'cnn_train.png'))\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn_eval = print_model_eval(cnn_pretrain)\n",
    "log['cnn'] = {\n",
    "    'eval' : cnn_eval,\n",
    "    'history' : cnn_pretrain_history.history\n",
    "}\n",
    "with open(os.path.join(OUTDIR, 'cnn_train_score.txt'), 'w') as f:\n",
    "    f.write(cnn_eval['train_report'])\n",
    "with open(os.path.join(OUTDIR, 'cnn_test_score.txt'), 'w') as f:\n",
    "    f.write(cnn_eval['test_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras_utils.save_model(os.path.join(OUTDIR, 'cnn.zip'), cnn_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save cnn weights\n",
    "weights = cnn_pretrain.get_weights()\n",
    "print [w.shape for w in weights]\n",
    "\n",
    "wimg = utils.make_mosaic_rgb(weights[0].transpose(0, 2, 3, 1), 7, 7)\n",
    "wimg[~wimg.mask] = utils.norm01(wimg[~wimg.mask])\n",
    "wimg[wimg.mask] = 0\n",
    "pl.figure(figsize=(10,10))\n",
    "pl.imshow(wimg, interpolation='nearest')\n",
    "pl.savefig(os.path.join(OUTDIR, 'cnn_weights.png'), dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# histnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histnn_pretrain = models.Graph()\n",
    "histnn_pretrain.scaler = scaler\n",
    "histnn_pretrain.add_input(name='ij', input_shape=(2,))\n",
    "\n",
    "histnn_pretrain.add_node(extract_win_layer, name='extract_windows', input='ij')\n",
    "\n",
    "# histnn\n",
    "histnn = models.Graph()\n",
    "histnn.add_input(name='win', input_shape=(3, winsize, winsize))\n",
    "histnn.add_node(keras_utils.ExtractHist1DLayer(nbins, binranges),\n",
    "           name='extract_hist', input='win')\n",
    "histnn.add_node(Flatten(), name='flatten', input='extract_hist')\n",
    "histnn.add_node(Dense(32, init=weights_init), name='dense1', input='flatten')\n",
    "histnn.add_node(Activation(activation), name='act1', input='dense1')\n",
    "histnn.add_node(Dropout(0.1), name='dropout1', input='act1')\n",
    "\n",
    "histnn.add_node(Dense(32, init=weights_init), name='dense2', input='dropout1')\n",
    "histnn.add_node(Activation(activation), name='act2', input='dense2')\n",
    "histnn.add_node(Dropout(0.25), name='dropout2', input='act2')\n",
    "histnn.add_output(name='histnn_output', input='dropout2')\n",
    "\n",
    "# -- pred\n",
    "histnn_pretrain.add_node(histnn, name='histnn', input='extract_windows')\n",
    "histnn_pretrain.add_node(Dense(nclasses, init=weights_init), name='dense2',\n",
    "                         input='histnn')\n",
    "histnn_pretrain.add_node(Activation('softmax'), name='softmax', input='dense2')\n",
    "histnn_pretrain.add_output(name='output', input='softmax')\n",
    "\n",
    "histnn_pretrain.compile('adam', {'output':'categorical_crossentropy'})\n",
    "\n",
    "print_layer_shapes(histnn_pretrain, input_shapes={'ij':train_ij[:42].shape})\n",
    "\n",
    "keras_utils.print_model_params(histnn_pretrain)\n",
    "\n",
    "histnn_pretrain_history = histnn_pretrain.fit(\n",
    "    {'ij': train_ij, 'output': Y_train},\n",
    "    batch_size=batch_size, nb_epoch=HISTNN_NEPOCHS,\n",
    "    verbose=KERAS_VERBOSE, shuffle=True\n",
    ")\n",
    "\n",
    "pl.figure()\n",
    "pl.title('Training loss')\n",
    "pl.plot(histnn_pretrain_history.epoch,\n",
    "        histnn_pretrain_history.history['loss'], label='loss')\n",
    "pl.savefig(os.path.join(OUTDIR, 'histnn_train.png'))\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histnn_eval = print_model_eval(histnn_pretrain)\n",
    "log['histnn'] = {\n",
    "    'eval' : histnn_eval,\n",
    "    'history' : histnn_pretrain_history.history\n",
    "}\n",
    "with open(os.path.join(OUTDIR, 'histnn_train_score.txt'), 'w') as f:\n",
    "    f.write(histnn_eval['train_report'])\n",
    "with open(os.path.join(OUTDIR, 'histnn_test_score.txt'), 'w') as f:\n",
    "    f.write(histnn_eval['test_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras_utils.save_model(os.path.join(OUTDIR, 'histnn.zip'), histnn_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_nn = models.Graph()\n",
    "merged_nn.scaler = scaler\n",
    "merged_nn.add_input(name='ij', input_shape=(2,))\n",
    "\n",
    "merged_nn.add_node(extract_win_layer, name='extract_windows', input='ij')\n",
    "\n",
    "merged_nn.add_node(histnn, name='histnn', input='extract_windows')\n",
    "merged_nn.add_node(cnn, name='cnn', input='extract_windows')\n",
    "\n",
    "merged_nn.add_node(Dense(128, init=weights_init), name='dense_merge_1',\n",
    "                   inputs=['histnn', 'cnn'], merge_mode='concat')\n",
    "merged_nn.add_node(Activation(activation), name='dense_act_1', input='dense_merge_1')\n",
    "merged_nn.add_node(Dropout(0.25), name='merge_dropout1', input='dense_act_1')\n",
    "merged_nn.add_node(Dense(nclasses, init=weights_init), name='dense2',\n",
    "                   input='merge_dropout1')\n",
    "merged_nn.add_node(Activation('softmax'), name='softmax', input='dense2')\n",
    "merged_nn.add_output(name='output', input='softmax')\n",
    "\n",
    "# Finetune by retraining the whole network with a low learning rate\n",
    "#opt = keras.optimizers.Adam(lr=0.0001)\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "merged_nn.compile(opt, {'output':'categorical_crossentropy'})\n",
    "\n",
    "print_layer_shapes(merged_nn, input_shapes={'ij':train_ij[:42].shape})\n",
    "\n",
    "keras_utils.print_model_params(merged_nn)\n",
    "\n",
    "merged_nn_history = merged_nn.fit(\n",
    "    {'ij': train_ij, 'output': Y_train},\n",
    "    batch_size=batch_size, nb_epoch=MERGED_NEPOCHS,\n",
    "    verbose=KERAS_VERBOSE, shuffle=True\n",
    ")\n",
    "\n",
    "pl.figure()\n",
    "pl.title('Training loss')\n",
    "pl.plot(merged_nn_history.epoch,\n",
    "        merged_nn_history.history['loss'], label='loss')\n",
    "pl.savefig(os.path.join(OUTDIR, 'merged_train.png'))\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_eval = print_model_eval(merged_nn)\n",
    "log['merged'] = {\n",
    "    'eval' : merged_eval,\n",
    "    'history' : merged_nn_history.history\n",
    "}\n",
    "with open(os.path.join(OUTDIR, 'merged_train_score.txt'), 'w') as f:\n",
    "    f.write(merged_eval['train_report'])\n",
    "with open(os.path.join(OUTDIR, 'merged_test_score.txt'), 'w') as f:\n",
    "    f.write(merged_eval['test_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras_utils.save_model(os.path.join(OUTDIR, 'merged_nn.zip'), merged_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save merged nn weights\n",
    "weights = merged_nn.get_weights()\n",
    "print [w.shape for w in weights]\n",
    "\n",
    "wimg = utils.make_mosaic_rgb(weights[4].transpose(0, 2, 3, 1), 7, 7)\n",
    "wimg[~wimg.mask] = utils.norm01(wimg[~wimg.mask])\n",
    "wimg[wimg.mask] = 0\n",
    "pl.figure(figsize=(10,10))\n",
    "pl.imshow(wimg, interpolation='nearest')\n",
    "pl.savefig(os.path.join(OUTDIR, 'merged_weights.png'), dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elapsed = time.time() - start_time\n",
    "print 'took %f [s]' % elapsed\n",
    "log['elapsed_time_s'] = elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.pickle_save(os.path.join(OUTDIR, 'log.pickle'), log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
